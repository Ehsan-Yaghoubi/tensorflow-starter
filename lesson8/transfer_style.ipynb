{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1fSu1MCrcCMb",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119.0
    },
    "outputId": "86ce3bc7-10b5-436a-a05e-da489608e6b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpg: keybox '/tmp/tmp0nsmqllh/pubring.gpg' created\n",
      "gpg: /tmp/tmp0nsmqllh/trustdb.gpg: trustdb created\n",
      "gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n",
      "gpg: Total number processed: 1\n",
      "gpg:               imported: 1\n",
      "Warning: apt-key output should not be parsed (stdout is not a terminal)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install a Drive FUSE wrapper.# Insta \n",
    "# https://github.com/astrada/google-drive-ocamlfuse\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "VAsm6IzTcNSb",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Generate auth tokens for Colab\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KTlCxY9ScR9B",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235.0
    },
    "outputId": "9a2968b9-57ad-4a59-f1e5-87368d64b7ae"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-98b1539cdc47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# https://github.com/jupyter/notebook/issues/3159\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetoutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n\\nEnter verification code: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "# Generate creds for the Drive FUSE library.# Gener \n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "# Work around misordering of STREAM and STDIN in Jupyter.\n",
    "# https://github.com/jupyter/notebook/issues/3159\n",
    "prompt = !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass(prompt[0] + '\\n\\nEnter verification code: ')\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "X9qcMkNhcWiY",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1003.0
    },
    "outputId": "ee98bf62-7e4f-4292-d581-0d5a0728f1c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in Drive:\n",
      "1115-resume-powerpoint-template-(cv-template).pdf\n",
      "4_6030726325214380780.odt\n",
      "AP-Workshop-Session2.odt\n",
      "Back transliteration.ods\n",
      "Colab Notebooks\n",
      "created.txt\n",
      "cross language survey.odt\n",
      "fra.txt\n",
      "google_analogy2.txt\n",
      "google_analogy.txt\n",
      "Homeworks-G2.ods\n",
      "IMG_20141110_092043~2.jpg\n",
      "Marks.ods\n",
      "myself.jpg\n",
      "NER & Word Clustering.ods\n",
      "New Doc 2.pdf\n",
      "Open source Tookits or Frameworks or ....ods\n",
      "Paper revision.odt\n",
      "para2vec.odt\n",
      "Persian corpus information.ods\n",
      "presentation\n",
      "resized_myself.jpg\n",
      "resized_starrynight.jpg\n",
      "simlex999-translated.xlsx\n",
      "simlex999-translated.xlsx.ods\n",
      "simlex-final.ods\n",
      "SNLP_PRESENTATION.pdf\n",
      "SNLP_PRESENTATION.pptx\n",
      "SourceCodes\n",
      "starrynight.jpg\n",
      "survey.odt\n",
      "Theoretical Interview.pdf\n",
      "transliteration.odt\n",
      "university position.ods\n",
      "Untitled document.odt\n",
      "Untitled document.odt (75ca89bf)\n",
      "Untitled presentation.pdf\n",
      "Untitled presentation.pdf (58a532f6)\n",
      "Useful things to know.ods\n",
      "Word embedding.ods\n",
      "آموزش ورد۲وک.odt\n",
      "برچسب سوالات.odt\n",
      "پرسشنامه ۲ (Responses).ods\n",
      "پرسشنامه ۲.zip\n",
      "پرسشنامه.zip\n",
      "جنسیت (Responses).ods\n",
      "ردیف.odt\n",
      "زمان تحویل حضوری.ods\n",
      "ساعت کار در کامنت.ods\n",
      "شناسنامه.pdf\n",
      "گزارش.odt\n",
      "لیست دانشجویان برنامه نویسی پیشرفته.ods\n",
      "موضوعات پروژه درس بازیابی اطلاعات.odt\n",
      "نسخه نهایی.pptx\n",
      "نسخه نهایی.pptx.pdf\n",
      "نمرات تمرین بازیابی اطلاعات.ods\n",
      "همکاری.odt\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive\n",
    "\n",
    "print ('Files in Drive:')\n",
    "!ls drive/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "G8bnl9sEY6df",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright 2018 Amir Hadifar. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "from PIL import Image, ImageOps\n",
    "from six.moves import urllib\n",
    "\n",
    "\n",
    "def download(download_link, file_name, expected_bytes):\n",
    "    \"\"\" Download the pretrained VGG-19 model if it's not already downloaded \"\"\"\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"VGG-19 pre-trained model is ready\")\n",
    "        return\n",
    "    print(\"Downloading the VGG pre-trained model. This might take a while ...\")\n",
    "    file_name, _ = urllib.request.urlretrieve(download_link, file_name)\n",
    "    file_stat = os.stat(file_name)\n",
    "    if file_stat.st_size == expected_bytes:\n",
    "        print('Successfully downloaded VGG-19 pre-trained model', file_name)\n",
    "    else:\n",
    "        raise Exception('File ' + file_name +\n",
    "                        ' might be corrupted. You should try downloading it with a browser.')\n",
    "\n",
    "\n",
    "def get_resized_image(img_path, width, height, save=True):\n",
    "    image = Image.open(img_path)\n",
    "    # PIL is column major so you have to swap the places of width and height\n",
    "    image = ImageOps.fit(image, (width, height), Image.ANTIALIAS)\n",
    "    if save:\n",
    "        image_dirs = img_path.split('/')\n",
    "        image_dirs[-1] = 'resized_' + image_dirs[-1]\n",
    "        out_path = '/'.join(image_dirs)\n",
    "        if not os.path.exists(out_path):\n",
    "            image.save(out_path)\n",
    "    image = np.asarray(image, np.float32)\n",
    "    return np.expand_dims(image, 0)\n",
    "\n",
    "\n",
    "def generate_noise_image(content_image, width, height, noise_ratio=0.6):\n",
    "    noise_image = np.random.uniform(-20, 20, (1, height, width, 3)).astype(np.float32)\n",
    "    return noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
    "\n",
    "\n",
    "def save_image(path, image):\n",
    "    image = image[0]\n",
    "    image = np.clip(image, 0, 255).astype('uint8')\n",
    "    scipy.misc.imsave(path, image)\n",
    "\n",
    "\n",
    "def safe_mkdir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "azdnayIUX-qr",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright 2018 Amir Hadifar. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "\n",
    "# VGG-19 parameters file\n",
    "VGG_DOWNLOAD_LINK = 'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat'\n",
    "VGG_FILENAME = 'imagenet-vgg-verydeep-19.mat'\n",
    "EXPECTED_BYTES = 534904783\n",
    "\n",
    "\n",
    "class VGG(object):\n",
    "    def __init__(self, input_img):\n",
    "        download(VGG_DOWNLOAD_LINK, VGG_FILENAME, EXPECTED_BYTES)\n",
    "        self.vgg_layers = scipy.io.loadmat(VGG_FILENAME)['layers']\n",
    "        self.input_img = input_img\n",
    "        self.mean_pixels = np.array([123.68, 116.779, 103.939]).reshape((1, 1, 1, 3))\n",
    "\n",
    "    def _weights(self, layer_idx, expected_layer_name):\n",
    "        \"\"\" Return the weights and biases at layer_idx already trained by VGG\n",
    "        \"\"\"\n",
    "        W = self.vgg_layers[0][layer_idx][0][0][2][0][0]\n",
    "        b = self.vgg_layers[0][layer_idx][0][0][2][0][1]\n",
    "        layer_name = self.vgg_layers[0][layer_idx][0][0][0][0]\n",
    "        assert layer_name == expected_layer_name\n",
    "        return W, b.reshape(b.size)\n",
    "\n",
    "    def conv2d_relu(self, prev_layer, layer_idx, layer_name):\n",
    "        \"\"\" Create a convolution layer with RELU using the weights and\n",
    "        biases extracted from the VGG model at 'layer_idx'. You should use\n",
    "        the function _weights() defined above to extract weights and biases.\n",
    "        _weights() returns numpy arrays, so you have to convert them to TF tensors.\n",
    "        Don't forget to apply relu to the output from the convolution.\n",
    "        Inputs:\n",
    "            prev_layer: the output tensor from the previous layer\n",
    "            layer_idx: the index to current layer in vgg_layers\n",
    "            layer_name: the string that is the name of the current layer.\n",
    "                        It's used to specify variable_scope.\n",
    "        Hint for choosing strides size:\n",
    "            for small images, you probably don't want to skip any pixel\n",
    "        \"\"\"\n",
    "        W, b = self._weights(layer_idx, layer_name)\n",
    "        W, b = tf.convert_to_tensor(W), tf.convert_to_tensor(b)\n",
    "        conv2d = tf.nn.conv2d(prev_layer, W, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        out = tf.nn.relu(tf.nn.bias_add(conv2d, b))\n",
    "        setattr(self, layer_name, out)\n",
    "\n",
    "    def avgpool(self, prev_layer, layer_name):\n",
    "        \"\"\" Create the average pooling layer. The paper suggests that\n",
    "        average pooling works better than max pooling.\n",
    "\n",
    "        Input:\n",
    "            prev_layer: the output tensor from the previous layer\n",
    "            layer_name: the string that you want to name the layer.\n",
    "                        It's used to specify variable_scope.\n",
    "        Hint for choosing strides and kszie: choose what you feel appropriate\n",
    "        \"\"\"\n",
    "        out = tf.nn.avg_pool(prev_layer, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"SAME\")\n",
    "        setattr(self, layer_name, out)\n",
    "\n",
    "    def load(self):\n",
    "        self.conv2d_relu(self.input_img, 0, 'conv1_1')\n",
    "        self.conv2d_relu(self.conv1_1, 2, 'conv1_2')\n",
    "        self.avgpool(self.conv1_2, 'avgpool1')\n",
    "        self.conv2d_relu(self.avgpool1, 5, 'conv2_1')\n",
    "        self.conv2d_relu(self.conv2_1, 7, 'conv2_2')\n",
    "        self.avgpool(self.conv2_2, 'avgpool2')\n",
    "        self.conv2d_relu(self.avgpool2, 10, 'conv3_1')\n",
    "        self.conv2d_relu(self.conv3_1, 12, 'conv3_2')\n",
    "        self.conv2d_relu(self.conv3_2, 14, 'conv3_3')\n",
    "        self.conv2d_relu(self.conv3_3, 16, 'conv3_4')\n",
    "        self.avgpool(self.conv3_4, 'avgpool3')\n",
    "        self.conv2d_relu(self.avgpool3, 19, 'conv4_1')\n",
    "        self.conv2d_relu(self.conv4_1, 21, 'conv4_2')\n",
    "        self.conv2d_relu(self.conv4_2, 23, 'conv4_3')\n",
    "        self.conv2d_relu(self.conv4_3, 25, 'conv4_4')\n",
    "        self.avgpool(self.conv4_4, 'avgpool4')\n",
    "        self.conv2d_relu(self.avgpool4, 28, 'conv5_1')\n",
    "        self.conv2d_relu(self.conv5_1, 30, 'conv5_2')\n",
    "        self.conv2d_relu(self.conv5_2, 32, 'conv5_3')\n",
    "        self.conv2d_relu(self.conv5_3, 34, 'conv5_4')\n",
    "        self.avgpool(self.conv5_4, 'avgpool5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "MmS1swT6MOop",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51.0
    },
    "outputId": "e2e24d6e-766a-43f6-92de-2faec4646a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘drive/outputs’: File exists\n",
      "adc.json  datalab  drive  imagenet-vgg-verydeep-19.mat\tsample_data\n"
     ]
    }
   ],
   "source": [
    "!mkdir drive/outputs\n",
    "!rm -r graphs\n",
    "!rm -r checkpoints\n",
    "!rm -r outputs\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "FKz4n_02bpPB",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1411.0
    },
    "outputId": "8a649b49-a899-42ac-f912-3bcd861926a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG-19 pre-trained model is ready\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/style-transfer-599\n",
      "Step 620\n",
      "   Sum: 31297414.4\n",
      "   Loss: 4506290.0\n",
      "   Took: 5.692676544189453 seconds\n",
      "Step 640\n",
      "   Sum: 31278147.5\n",
      "   Loss: 4451164.0\n",
      "   Took: 4.223987817764282 seconds\n",
      "Step 660\n",
      "   Sum: 31259466.1\n",
      "   Loss: 4398464.5\n",
      "   Took: 5.4752585887908936 seconds\n",
      "Step 680\n",
      "   Sum: 31240661.8\n",
      "   Loss: 4346468.0\n",
      "   Took: 3.9004404544830322 seconds\n",
      "Step 700\n",
      "   Sum: 31221686.6\n",
      "   Loss: 4294899.5\n",
      "   Took: 4.532842397689819 seconds\n",
      "Step 720\n",
      "   Sum: 31202499.3\n",
      "   Loss: 4243518.0\n",
      "   Took: 4.796046733856201 seconds\n",
      "Step 740\n",
      "   Sum: 31183136.5\n",
      "   Loss: 4192772.5\n",
      "   Took: 4.985067367553711 seconds\n",
      "Step 760\n",
      "   Sum: 31163538.9\n",
      "   Loss: 4142727.5\n",
      "   Took: 4.026298761367798 seconds\n",
      "Step 780\n",
      "   Sum: 31143721.8\n",
      "   Loss: 4093217.8\n",
      "   Took: 4.055847883224487 seconds\n",
      "Step 800\n",
      "   Sum: 31123715.5\n",
      "   Loss: 4044229.0\n",
      "   Took: 7.791335105895996 seconds\n",
      "Step 820\n",
      "   Sum: 31103527.9\n",
      "   Loss: 3995635.5\n",
      "   Took: 4.406207799911499 seconds\n",
      "Step 840\n",
      "   Sum: 31083235.2\n",
      "   Loss: 3947272.2\n",
      "   Took: 6.312697172164917 seconds\n",
      "Step 860\n",
      "   Sum: 31062778.2\n",
      "   Loss: 3899292.8\n",
      "   Took: 4.024611473083496 seconds\n",
      "Step 880\n",
      "   Sum: 31042157.6\n",
      "   Loss: 3851987.8\n",
      "   Took: 3.9740729331970215 seconds\n",
      "Step 900\n",
      "   Sum: 31021405.3\n",
      "   Loss: 3805304.5\n",
      "   Took: 4.290274143218994 seconds\n",
      "Step 920\n",
      "   Sum: 31000485.3\n",
      "   Loss: 3759353.2\n",
      "   Took: 4.049139022827148 seconds\n",
      "Step 940\n",
      "   Sum: 30979419.8\n",
      "   Loss: 3713881.2\n",
      "   Took: 4.195723295211792 seconds\n",
      "Step 960\n",
      "   Sum: 30958232.9\n",
      "   Loss: 3668944.5\n",
      "   Took: 4.362659215927124 seconds\n",
      "Step 980\n",
      "   Sum: 30936911.2\n",
      "   Loss: 3624300.5\n",
      "   Took: 4.0877203941345215 seconds\n",
      "Step 1000\n",
      "   Sum: 30915467.5\n",
      "   Loss: 3579848.2\n",
      "   Took: 4.216723918914795 seconds\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright 2018 Amir Hadifar. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def setup():\n",
    "    safe_mkdir('checkpoints')\n",
    "    safe_mkdir('outputs')\n",
    "\n",
    "\n",
    "class StyleTransfer(object):\n",
    "    def __init__(self, content_img, style_img, img_width, img_height):\n",
    "        \"\"\"\n",
    "        img_width and img_height are the dimensions we expect from the generated image.\n",
    "        We will resize input content image and input style image to match this dimension.\n",
    "        Feel free to alter any hyperparameter here and see how it affects your training.\n",
    "        \"\"\"\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.content_img = get_resized_image(content_img, img_width, img_height)\n",
    "        self.style_img = get_resized_image(style_img, img_width, img_height)\n",
    "        self.initial_img = generate_noise_image(self.content_img, img_width, img_height)\n",
    "\n",
    "        self.content_layer = 'conv4_2'\n",
    "        self.style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
    "        # content_w, style_w: corresponding weights for content loss and style loss\n",
    "        self.content_w = 0.01\n",
    "        self.style_w = 1\n",
    "        # style_layer_w: weights for different style layers. deep layers have more weights\n",
    "        self.style_layer_w = [0.5, 1.0, 1.5, 3.0, 4.0]\n",
    "        self.gstep = tf.get_variable(name='global_step', initializer=0, trainable=False)  # global step\n",
    "        self.lr = 1.0\n",
    "\n",
    "    def create_input(self):\n",
    "        \"\"\"\n",
    "        We will use one input_img as a placeholder for the content image,\n",
    "        style image, and generated image, because:\n",
    "            1. they have the same dimension\n",
    "            2. we have to extract the same set of features from them\n",
    "        We use a variable instead of a placeholder because we're, at the same time,\n",
    "        training the generated image to get the desirable result.\n",
    "        Note: image height corresponds to number of rows, not columns.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('input'):\n",
    "            self.input_img = tf.get_variable('in_img',\n",
    "                                             shape=([1, self.img_height, self.img_width, 3]),\n",
    "                                             dtype=tf.float32,\n",
    "                                             initializer=tf.zeros_initializer())\n",
    "\n",
    "    def load_vgg(self):\n",
    "        \"\"\"\n",
    "        Load the saved model parameters of VGG-19, using the input_img\n",
    "        as the input to compute the output at each layer of vgg.\n",
    "        During training, VGG-19 mean-centered all images and found the mean pixels\n",
    "        to be [123.68, 116.779, 103.939] along RGB dimensions. We have to subtract\n",
    "        this mean from our images.\n",
    "        \"\"\"\n",
    "        self.vgg = VGG(self.input_img)\n",
    "        self.vgg.load()\n",
    "        self.content_img -= self.vgg.mean_pixels\n",
    "        self.style_img -= self.vgg.mean_pixels\n",
    "\n",
    "    def _content_loss(self, P, F):\n",
    "        \"\"\" Calculate the loss between the feature representation of the\n",
    "        content image and the generated image.\n",
    "\n",
    "        Inputs:\n",
    "            P: content representation of the content image\n",
    "            F: content representation of the generated image\n",
    "            Read the assignment handout for more details\n",
    "            Note: Don't use the coefficient 0.5 as defined in the paper.\n",
    "            Use the coefficient defined in the assignment handout.\n",
    "        \"\"\"\n",
    "        self.content_loss = tf.reduce_sum(tf.square(F - P)) / (4 * P.size)\n",
    "\n",
    "    def _gram_matrix(self, F, N, M):\n",
    "        \"\"\" Create and return the gram matrix for tensor F\n",
    "            Hint: you'll first have to reshape F\n",
    "        \"\"\"\n",
    "        # N third dim of feature map\n",
    "        # M product of first two dim of feature map\n",
    "        # F feature map\n",
    "        F = tf.reshape(F, [M, N])\n",
    "        return tf.matmul(F, F, transpose_a=True)\n",
    "\n",
    "    def _single_style_loss(self, a, g):\n",
    "        \"\"\" Calculate the style loss at a certain layer\n",
    "        Inputs:\n",
    "            a is the feature representation of the style image at that layer\n",
    "            g is the feature representation of the generated image at that layer\n",
    "        Output:\n",
    "            the style loss at a certain layer (which is E_l in the paper)\n",
    "        Hint: 1. you'll have to use the function _gram_matrix()\n",
    "            2. we'll use the same coefficient for style loss as in the paper\n",
    "            3. a and g are feature representation, not gram matrices\n",
    "        \"\"\"\n",
    "        # M is the product of the first two dimensions of the feature map\n",
    "        # N third dimension of the feature map\n",
    "        N = a.shape[3]\n",
    "        M = a.shape[1] * a.shape[2]\n",
    "        A = self._gram_matrix(a, N=N, M=M)\n",
    "        G = self._gram_matrix(g, N=N, M=M)\n",
    "        coeff = 1 / (4 * (N ** 2) * (M ** 2))\n",
    "        return coeff * tf.reduce_sum(tf.square(G - A))\n",
    "\n",
    "    def _style_loss(self, A):\n",
    "        \"\"\" Calculate the total style loss as a weighted sum\n",
    "        of style losses at all style layers\n",
    "        Hint: you'll have to use _single_style_loss()\n",
    "        \"\"\"\n",
    "        n_layers = len(A)\n",
    "        E = [self._single_style_loss(A[i], getattr(self.vgg, self.style_layers[i])) for i in range(n_layers)]\n",
    "        self.style_loss = tf.reduce_sum([self.style_layer_w[i] * E[i] for i in range(n_layers)])\n",
    "\n",
    "    def losses(self):\n",
    "        with tf.variable_scope('losses'):\n",
    "            with tf.Session() as sess:\n",
    "                # assign content image to the input variable\n",
    "                sess.run(self.input_img.assign(self.content_img))\n",
    "                gen_img_content = getattr(self.vgg, self.content_layer)\n",
    "                content_img_content = sess.run(gen_img_content)\n",
    "            self._content_loss(content_img_content, gen_img_content)\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(self.input_img.assign(self.style_img))\n",
    "                style_layers = sess.run([getattr(self.vgg, layer) for layer in self.style_layers])\n",
    "            self._style_loss(style_layers)\n",
    "\n",
    "            self.total_loss = self.style_w * self.style_loss + self.content_w * self.content_loss\n",
    "\n",
    "    def optimize(self):\n",
    "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.total_loss, global_step=self.gstep)\n",
    "\n",
    "    def create_summary(self):\n",
    "        with tf.name_scope('summary'):\n",
    "            tf.summary.scalar('total_loss', self.total_loss)\n",
    "            tf.summary.scalar('content_loss', self.content_loss)\n",
    "            tf.summary.scalar('style_loss', self.style_loss)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def build(self):\n",
    "        self.create_input()\n",
    "        self.load_vgg()\n",
    "        self.losses()\n",
    "        self.optimize()\n",
    "        self.create_summary()\n",
    "\n",
    "    def train(self, n_iters):\n",
    "        skip_step = 1\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            writer = tf.summary.FileWriter('graphs/transfer-style/lr' + str(self.lr), sess.graph)\n",
    "\n",
    "            sess.run(self.input_img.assign(self.initial_img))\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "            initial_step = self.gstep.eval()\n",
    "\n",
    "            start_time = time.time()\n",
    "            for index in range(initial_step, n_iters):\n",
    "                if 5 <= index < 20:\n",
    "                    skip_step = 10\n",
    "                elif index >= 20:\n",
    "                    skip_step = 20\n",
    "\n",
    "                sess.run(self.opt)\n",
    "                if (index + 1) % skip_step == 0:\n",
    "                    gen_image, total_loss, summary = sess.run([self.input_img, self.total_loss, self.summary_op])\n",
    "\n",
    "                    # add back the mean pixels we subtracted before\n",
    "                    gen_image = gen_image + self.vgg.mean_pixels\n",
    "                    writer.add_summary(summary, global_step=index)\n",
    "                    print('Step {}\\n   Sum: {:5.1f}'.format(index + 1, np.sum(gen_image)))\n",
    "                    print('   Loss: {:5.1f}'.format(total_loss))\n",
    "                    print('   Took: {} seconds'.format(time.time() - start_time))\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    filename = 'drive/outputs/%d.png' % index\n",
    "                    save_image(filename, gen_image)\n",
    "\n",
    "                    if (index + 1) % 20 == 0:\n",
    "                        saver.save(sess, 'checkpoints/style-transfer', global_step=index)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    setup()\n",
    "    machine = StyleTransfer('drive/myself.jpg', 'drive/starrynight.jpg', 333, 250)\n",
    "    machine.build()\n",
    "    machine.train(1000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "transfer style.ipynb",
   "version": "0.3.2",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
