{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "bTtgQTmslh8D",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DataHelper:\n",
    "    def __init__(self, data, label):\n",
    "        self._index_in_epoch = 0\n",
    "        self._epochs_completed = 0\n",
    "        self._data = data\n",
    "        self._label = label\n",
    "        self._num_examples = data.shape[0]\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    @property\n",
    "    def label(self):\n",
    "        return self._label\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        start = self._index_in_epoch\n",
    "        if start == 0 and self._epochs_completed == 0:\n",
    "            idx = np.arange(0, self._num_examples)  # get all possible indexes\n",
    "            np.random.shuffle(idx)  # shuffle index\n",
    "            self._data = self.data[idx]  # get list of `num` random samples\n",
    "            self._label = self.label[idx]\n",
    "\n",
    "        # go to the next batch\n",
    "        if start + batch_size > self._num_examples:\n",
    "            self._epochs_completed += 1\n",
    "            rest_num_examples = self._num_examples - start\n",
    "            data_rest_part = self.data[start:self._num_examples]\n",
    "            label_rest_part = self.label[start:self._num_examples]\n",
    "            idx0 = np.arange(0, self._num_examples)  # get all possible indexes\n",
    "            np.random.shuffle(idx0)  # shuffle indexes\n",
    "            self._data = self.data[idx0]  # get list of `num` random samples\n",
    "            self._label = self._label[idx0]\n",
    "            start = 0\n",
    "            # avoid the case where the #sample != integer times of batch_size\n",
    "            self._index_in_epoch = batch_size - rest_num_examples\n",
    "            end = self._index_in_epoch\n",
    "            data_new_part = self._data[start:end]\n",
    "            label_new_part = self._label[start:end]\n",
    "            return np.concatenate((data_rest_part, data_new_part), axis=0), np.concatenate(\n",
    "                (label_rest_part, label_new_part), axis=0)\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            return self._data[start:end], self._label[start:end]\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "zW5qX27tlbtb",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1037.0
    },
    "outputId": "1d1731f4-247a-4854-83e0-db2fabebe6a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "training::: accuracy 0.12 loss 2.3178961\n",
      "testing::: accuracy 0.106 loss 2.3341513\n",
      "--------------------------------------------------\n",
      "100\n",
      "training::: accuracy 0.14 loss 2.2729654\n",
      "testing::: accuracy 0.174 loss 2.259501\n",
      "--------------------------------------------------\n",
      "200\n",
      "training::: accuracy 0.21 loss 2.2197733\n",
      "testing::: accuracy 0.173 loss 2.209201\n",
      "--------------------------------------------------\n",
      "300\n",
      "training::: accuracy 0.18 loss 2.1289027\n",
      "testing::: accuracy 0.2 loss 2.1528516\n",
      "--------------------------------------------------\n",
      "400\n",
      "training::: accuracy 0.2 loss 2.0882955\n",
      "testing::: accuracy 0.226 loss 2.0963974\n",
      "--------------------------------------------------\n",
      "500\n",
      "training::: accuracy 0.22 loss 2.1009302\n",
      "testing::: accuracy 0.234 loss 2.071723\n",
      "--------------------------------------------------\n",
      "600\n",
      "training::: accuracy 0.22 loss 2.0844135\n",
      "testing::: accuracy 0.24 loss 2.0511773\n",
      "--------------------------------------------------\n",
      "700\n",
      "training::: accuracy 0.2 loss 2.0890048\n",
      "testing::: accuracy 0.256 loss 2.0379088\n",
      "--------------------------------------------------\n",
      "800\n",
      "training::: accuracy 0.25 loss 2.0522103\n",
      "testing::: accuracy 0.26 loss 2.0236144\n",
      "--------------------------------------------------\n",
      "900\n",
      "training::: accuracy 0.25 loss 1.9923402\n",
      "testing::: accuracy 0.275 loss 2.0011911\n",
      "--------------------------------------------------\n",
      "1000\n",
      "training::: accuracy 0.26 loss 2.0824099\n",
      "testing::: accuracy 0.28 loss 1.9857783\n",
      "--------------------------------------------------\n",
      "1100\n",
      "training::: accuracy 0.25 loss 1.9961704\n",
      "testing::: accuracy 0.287 loss 1.9808669\n",
      "--------------------------------------------------\n",
      "1200\n",
      "training::: accuracy 0.29 loss 1.9722916\n",
      "testing::: accuracy 0.315 loss 1.956502\n",
      "--------------------------------------------------\n",
      "1300\n",
      "training::: accuracy 0.25 loss 1.9174817\n",
      "testing::: accuracy 0.311 loss 1.9478191\n",
      "--------------------------------------------------\n",
      "1400\n",
      "training::: accuracy 0.27 loss 2.0261488\n",
      "testing::: accuracy 0.297 loss 1.9452175\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x_train, x_test = x_train / 255, x_test / 255\n",
    "# x_train = x_train[:20]\n",
    "# x_test = x_test[:20]\n",
    "# y_train = y_train[:20]\n",
    "# y_test = y_test[:20]\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "data_helper = DataHelper(x_train, y_train)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 32, 32, 3], name='image')\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10], name='label')\n",
    "pKeep = tf.placeholder(tf.float32)\n",
    "\n",
    "K = 6\n",
    "L = 12\n",
    "M = 18\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 3, K], stddev=0.1))\n",
    "B1 = tf.Variable(tf.ones([K]) / 10)\n",
    "W2 = tf.Variable(tf.truncated_normal([4, 4, K, L], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([L]) / 10)\n",
    "W3 = tf.Variable(tf.truncated_normal([3, 3, L, M], stddev=0.1))\n",
    "B3 = tf.Variable(tf.ones([M]) / 10)\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([18 * 8 * 8, 200], stddev=0.1))\n",
    "B4 = tf.Variable(tf.ones([200]) / 10)\n",
    "W5 = tf.Variable(tf.truncated_normal([200, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.ones([10]) / 10)\n",
    "\n",
    "Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME') + B1)\n",
    "Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, 2, 2, 1], padding=\"SAME\") + B2)\n",
    "Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, 2, 2, 1], padding=\"SAME\") + B3)\n",
    "Y3 = tf.reshape(Y3, [-1, M * 8 * 8])\n",
    "Y3 = tf.nn.dropout(Y3, pKeep)\n",
    "Y4 = tf.nn.relu(tf.matmul(Y3, W4) + B4)\n",
    "y_logit = tf.matmul(Y4, W5) + B5\n",
    "Y_pred = tf.nn.softmax(y_logit)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_logit, labels=Y)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(Y_pred, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.004).minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(25000):\n",
    "    batch_X, batch_Y = data_helper.next_batch(100)\n",
    "\n",
    "    train_data = {X: batch_X, Y: batch_Y, pKeep: 0.75}\n",
    "    sess.run(train_step, feed_dict=train_data)\n",
    "\n",
    "    acc_train, loss_train = sess.run([accuracy, cross_entropy], feed_dict=train_data)\n",
    "    test_data = {X: x_test[:1000], Y: y_test[:1000], pKeep: 1.0}\n",
    "    acc_test, loss_test = sess.run([accuracy, cross_entropy], feed_dict=test_data)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "        print(\"training::: accuracy\", acc_train, \"loss\", loss_train)\n",
    "        print(\"testing::: accuracy\", acc_test, \"loss\", loss_test)\n",
    "        print(50 * '-')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mnist_tf.ipynb",
   "version": "0.3.2",
   "views": {},
   "default_view": {},
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
